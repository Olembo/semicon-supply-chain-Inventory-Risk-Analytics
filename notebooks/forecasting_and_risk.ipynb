{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d384ba43",
   "metadata": {},
   "source": [
    "### Aggregate Historical Demand per Component (from CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Paths ----\n",
    "os.makedirs(\"clean_data\", exist_ok=True)\n",
    "CLEAN_PO = \"clean_data/production_orders_clean.csv\"\n",
    "RAW_PO   = \"raw_data/production_orders.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Load production orders ----\n",
    "po_path = CLEAN_PO if os.path.exists(CLEAN_PO) else RAW_PO\n",
    "po = pd.read_csv(po_path, parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity: required columns\n",
    "req_cols = {\"date\", \"component_id\", \"units_required\", \"units_issued\"}\n",
    "missing = req_cols - set(po.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in {po_path}: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e18b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- DAILY aggregation ----\n",
    "daily = (\n",
    "    po.groupby([\"component_id\", \"date\"], as_index=False)\n",
    "      .agg(actual_units=(\"units_required\", \"sum\"),\n",
    "           actual_issued=(\"units_issued\", \"sum\"))\n",
    "      .sort_values([\"component_id\", \"date\"])\n",
    ")\n",
    "daily.to_csv(\"clean_data/historical_demand_daily.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- MONTHLY aggregation with complete month range per component ----\n",
    "daily[\"month\"] = daily[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "monthly = (\n",
    "    daily.groupby([\"component_id\", \"month\"], as_index=False)\n",
    "         .agg(actual_units=(\"actual_units\", \"sum\"),\n",
    "              actual_issued=(\"actual_issued\", \"sum\"))\n",
    "         .sort_values([\"component_id\", \"month\"])\n",
    ")\n",
    "\n",
    "def _complete_months(df_comp: pd.DataFrame) -> pd.DataFrame:\n",
    "    comp_id = df_comp[\"component_id\"].iloc[0]\n",
    "    full_idx = pd.period_range(df_comp[\"month\"].min(), df_comp[\"month\"].max(), freq=\"M\")\n",
    "    df_comp = df_comp.set_index(\"month\").reindex(full_idx)\n",
    "    df_comp.index.name = \"month\"\n",
    "    df_comp[\"component_id\"] = comp_id\n",
    "    df_comp[[\"actual_units\", \"actual_issued\"]] = df_comp[[\"actual_units\", \"actual_issued\"]].fillna(0)\n",
    "    return df_comp.reset_index()\n",
    "\n",
    "monthly_full = (\n",
    "    monthly.groupby(\"component_id\", group_keys=False)\n",
    "           .apply(_complete_months)\n",
    "           .sort_values([\"component_id\", \"month\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Period to 'YYYY-MM' string for portability\n",
    "monthly_full[\"month\"] = monthly_full[\"month\"].astype(str)\n",
    "\n",
    "monthly_full.to_csv(\"clean_data/historical_demand_monthly.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - clean_data/historical_demand_daily.csv\")\n",
    "print(\" - clean_data/historical_demand_monthly.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b47a6",
   "metadata": {},
   "source": [
    "### Rolling Averages & Seasonal Decomposition (STL) per Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f489d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "os.makedirs(\"clean_data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e41b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load monthly demand ---\n",
    "path = \"clean_data/historical_demand_monthly.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required columns\n",
    "required = {\"component_id\", \"month\", \"actual_units\", \"actual_issued\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in {path}: {missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize month to Timestamp (month start)\n",
    "df[\"month\"] = pd.to_datetime(df[\"month\"], format=\"%Y-%m\")\n",
    "\n",
    "# Sort for stable rolling/decomposition\n",
    "df = df.sort_values([\"component_id\", \"month\"]).reset_index(drop=True)\n",
    "\n",
    "# Container for results\n",
    "out_parts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each component separately\n",
    "for comp_id, g in df.groupby(\"component_id\", sort=False):\n",
    "    g = g.sort_values(\"month\").copy()\n",
    "    g.set_index(\"month\", inplace=True)\n",
    "\n",
    "    # Rolling features (centered to reduce phase lag, with min periods)\n",
    "    g[\"ra_3m\"]  = g[\"actual_units\"].rolling(window=3, min_periods=2, center=True).mean()\n",
    "    g[\"ra_6m\"]  = g[\"actual_units\"].rolling(window=6, min_periods=3, center=True).mean()\n",
    "    g[\"rstd_6m\"] = g[\"actual_units\"].rolling(window=6, min_periods=3, center=True).std()\n",
    "\n",
    "    # STL decomposition (monthly seasonality = 12)\n",
    "    n = len(g)\n",
    "    if n >= 12 and g[\"actual_units\"].notna().sum() >= 12:\n",
    "        # Fill small gaps for STL stability\n",
    "        series = g[\"actual_units\"].astype(float).interpolate(limit_direction=\"both\")\n",
    "        try:\n",
    "            stl = STL(series, period=12, robust=True)\n",
    "            res = stl.fit()\n",
    "            g[\"stl_trend\"]    = res.trend\n",
    "            g[\"stl_seasonal\"] = res.seasonal\n",
    "            g[\"stl_resid\"]    = res.resid\n",
    "        except Exception:\n",
    "            # If STL fails for any reason, leave NaNs\n",
    "            g[\"stl_trend\"] = np.nan\n",
    "            g[\"stl_seasonal\"] = np.nan\n",
    "            g[\"stl_resid\"] = np.nan\n",
    "    else:\n",
    "        g[\"stl_trend\"] = np.nan\n",
    "        g[\"stl_seasonal\"] = np.nan\n",
    "        g[\"stl_resid\"] = np.nan\n",
    "\n",
    "    g[\"component_id\"] = comp_id\n",
    "    out_parts.append(g.reset_index())\n",
    "\n",
    "# Concatenate and save\n",
    "features = pd.concat(out_parts, axis=0, ignore_index=True)\n",
    "\n",
    "# Order and tidy columns\n",
    "cols = [\n",
    "    \"component_id\", \"month\", \"actual_units\", \"actual_issued\",\n",
    "    \"ra_3m\", \"ra_6m\", \"rstd_6m\",\n",
    "    \"stl_trend\", \"stl_seasonal\", \"stl_resid\"\n",
    "]\n",
    "features = features[cols].sort_values([\"component_id\", \"month\"]).copy()\n",
    "\n",
    "# Save\n",
    "out_path = \"clean_data/demand_rolling_stl_features.csv\"\n",
    "features.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\" Rolling & STL features saved to: {out_path}\")\n",
    "print(features.head(8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6cbf9d",
   "metadata": {},
   "source": [
    "### Repair supply inputs & recompute risk flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Paths & helpers\n",
    "\n",
    "base_clean = Path(\"cleaned_data\") if Path(\"cleaned_data\").exists() else Path(\"clean_data\")\n",
    "base_raw   = Path(\"raw_data\")\n",
    "\n",
    "def load_csv_safe(path_primary: Path, path_fallback: Path = None, parse_cols=None):\n",
    "    if path_primary and path_primary.exists():\n",
    "        return pd.read_csv(path_primary, parse_dates=parse_cols or [])\n",
    "    if path_fallback and path_fallback.exists():\n",
    "        return pd.read_csv(path_fallback, parse_dates=parse_cols or [])\n",
    "    raise FileNotFoundError(f\"Neither {path_primary} nor {path_fallback} found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Forecast frame\n",
    "try:\n",
    "    _ = forecast_df  # if already in memory\n",
    "except NameError:\n",
    "    # Load prior output just to get forecasts\n",
    "    fc_path = base_clean / \"forecast_next_month_risk.csv\"\n",
    "    _temp = pd.read_csv(fc_path)\n",
    "    # Keep only forecast inputs\n",
    "    needed_cols = [\"component_id\", \"forecast_month\", \"forecast_units\"]\n",
    "    extra = [c for c in [\"method\"] if c in _temp.columns]\n",
    "    forecast_df = _temp[needed_cols + extra].copy()\n",
    "\n",
    "# Ensure types\n",
    "forecast_df[\"forecast_units\"] = forecast_df[\"forecast_units\"].astype(float).clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load inventory & deliveries\n",
    "# Inventory levels\n",
    "inv = load_csv_safe(\n",
    "    base_clean / \"inventory_levels_clean.csv\",\n",
    "    base_raw   / \"inventory_levels.csv\",\n",
    "    parse_cols=[\"date\"]\n",
    ")\n",
    "\n",
    "# Deliveries\n",
    "dl = load_csv_safe(\n",
    "    base_clean / \"delivery_logs_clean.csv\",\n",
    "    base_raw   / \"delivery_logs.csv\",\n",
    "    parse_cols=[\"order_date\",\"expected_delivery_date\",\"actual_delivery_date\"]\n",
    ")\n",
    "\n",
    "# Sanity columns\n",
    "if \"closing_stock\" not in inv.columns:\n",
    "    # support alternative naming, else error\n",
    "    alt = [c for c in inv.columns if c.lower() == \"closing_stock\"]\n",
    "    if alt:\n",
    "        inv.rename(columns={alt[0]: \"closing_stock\"}, inplace=True)\n",
    "    else:\n",
    "        raise KeyError(\"inventory_levels is missing 'closing_stock' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Latest non-zero stock snapshot (last 90 days fallback)\n",
    "today = pd.Timestamp.today().normalize()\n",
    "lookback_start = today - pd.Timedelta(days=90)\n",
    "\n",
    "# restrict to last 90 days if available\n",
    "inv_recent = inv[inv[\"date\"] >= inv[\"date\"].max() - pd.Timedelta(days=120)].copy()\n",
    "\n",
    "def last_nonzero_stock(g: pd.DataFrame) -> int:\n",
    "    g = g.sort_values(\"date\")\n",
    "    # try last non-zero in last 90 days\n",
    "    g90 = g[g[\"date\"] >= lookback_start]\n",
    "    nz = g90[g90[\"closing_stock\"] > 0][\"closing_stock\"]\n",
    "    if not nz.empty:\n",
    "        return int(nz.iloc[-1])\n",
    "    # fallback: latest value overall (may be zero)\n",
    "    return int(g[\"closing_stock\"].iloc[-1])\n",
    "\n",
    "latest_stock = (\n",
    "    inv_recent.groupby(\"component_id\", as_index=False)\n",
    "             .apply(lambda x: pd.Series({\"latest_closing_stock\": last_nonzero_stock(x)}))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74bcc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Inbound in next 30 days from TODAY\n",
    "window_end = today + pd.Timedelta(days=30)\n",
    "\n",
    "# prefer quantity_received; fallback to quantity_ordered\n",
    "qty_col = \"quantity_received\" if \"quantity_received\" in dl.columns else \"quantity_ordered\"\n",
    "if qty_col not in dl.columns:\n",
    "    raise KeyError(\"delivery_logs missing 'quantity_received' and 'quantity_ordered' columns.\")\n",
    "\n",
    "inbound_mask = (\n",
    "    (dl[\"expected_delivery_date\"] >= today) &\n",
    "    (dl[\"expected_delivery_date\"] <= window_end) &\n",
    "    ( dl[\"actual_delivery_date\"].isna() | (dl[\"actual_delivery_date\"] > today) )\n",
    ")\n",
    "\n",
    "inbound_30 = (\n",
    "    dl.loc[inbound_mask]\n",
    "      .groupby(\"component_id\", as_index=False)[qty_col]\n",
    "      .sum()\n",
    "      .rename(columns={qty_col: \"inbound_next_30\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf484d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Merge & recompute coverage/risk\n",
    "risk = (\n",
    "    forecast_df.merge(latest_stock, on=\"component_id\", how=\"left\")\n",
    "               .merge(inbound_30, on=\"component_id\", how=\"left\")\n",
    "               .copy()\n",
    ")\n",
    "\n",
    "risk[\"latest_closing_stock\"] = risk[\"latest_closing_stock\"].fillna(0).clip(lower=0).astype(int)\n",
    "risk[\"inbound_next_30\"] = risk[\"inbound_next_30\"].fillna(0).astype(int)\n",
    "\n",
    "risk[\"coverage_ratio\"] = np.where(\n",
    "    risk[\"forecast_units\"] > 0,\n",
    "    (risk[\"latest_closing_stock\"] + risk[\"inbound_next_30\"]) / risk[\"forecast_units\"],\n",
    "    np.inf\n",
    ")\n",
    "\n",
    "risk[\"risk_flag\"] = pd.cut(\n",
    "    risk[\"coverage_ratio\"],\n",
    "    bins=[-np.inf, 0.8, 1.0, np.inf],\n",
    "    labels=[\"High\", \"Medium\", \"Low\"]\n",
    ")\n",
    "\n",
    "\n",
    "# 6) Save\n",
    "out_dir = base_clean if base_clean.exists() else Path(\"clean_data\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "out_path = out_dir / \"forecast_next_month_risk.csv\"\n",
    "risk.sort_values([\"risk_flag\",\"coverage_ratio\",\"component_id\"]).to_csv(out_path, index=False)\n",
    "\n",
    "print(f\" Recomputed risk table saved to: {out_path}\")\n",
    "print(risk.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
